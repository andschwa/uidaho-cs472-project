* TODO Project #1a Hill Climbing and Simulated Annealing
   DEADLINE: <2014-02-05 Wed>
This is the first subproject of the first project. The goal of this
project is to write and test a hillclimbing and simulated annealing
search algorithms for two of the benchmark optimization problems.

For this subproject you only need to work on the Spherical and
Schwefel functions, defined [[http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume24/ortizboyer05a-html/node6.html#tabla:DefFunc][here]]. (Note the first function labeled as
Schwefel on this page is actually the double sum, which we are not
using. We are using the Schwefel function defined immediately after
the Rastigin function.)

Pay careful attention to the ranges of the functions. You will want to
use those ranges both in creating intial individuals and in
controlling the generation of neighbors, e.g. you don't want your GA
'wondering' out of the search space. Note that here the functions are
all defined with 30 dimensions, e.g. P = 30 in the function
definitions.
** Task
Write a hill climbing algorithm and a simulated annealing algorithm to
find the input values (x_{1}, ... ,x_{30}) that minimizes the two test
function.
** Write-up
Write a short paper describing the results of your project that
includes the following sections:

- Algorithm descriptions :: Description of the two algorithms. Be
     careful to include all of the details someone would need to
     replicate your work: how neightbors are generated in hill
     climbing, what the temperature schedule is for simulated
     annealing, etc. .
- Results :: Table showing the results for both algorithms on both
             test functions.
- Conclusions :: If its not working, why not. And what are then next
                 steps to complete the project.

* Functions
** Spherical
f_{Sph}(x) = (\sum_{i=1})^{p} (x_{i})^{2}
x_{i} \in [-5.12, 5.12]
x^{\*} = (0, 0, ..., 0); f_{Sph}(x^{\*}) = 0

Use fewer random restarts, more neighbors
** Schwefel
f_{Sch}(x) = 418.9829 \cdot p + (\sum_{i=1})^{p} x_{i }sin(\radic|x_{i}|)
x_{i} \in [-512.03, 511.97]
x^{\*} = (-420.9687, ..., -420.9687); f_{Sch}(x^{\*}) = 0

Use more random restarts, fewer neighbors
* Algorithms
** Hill Climbing
- generate a random solution s_{1}
- do random restart until a good enough solution is found
  - do until no better neighbor
    - pick a neighbor solution s_{2}
    - if s_{2} is better than s_{1}
      - s_{1} \gets s_{2}
    - loop
  - loop
** Simulated Annealing
- pick a random solution s_{1}
- for T = 100 to 0 step -.1
  - pick a neighbor of s_{1}, s_{2}
  - if s_{2} is better than s_{1}
    - s_{1} \gets s_{2}
  - else
    - with probability P(e_{1}, e_{2}, T)
    - s_{1} \gets s_{2} anyway
    - where e_{1} is the fitness/energy of s, e_{2} of 2

- P(e_{1}, e_{2} T) = e^{-c(e_{1} - e_{2})/T} = 1/e^{(e_{1} - e_{2})/T}
- Scaling constant c to adjust probabilites

- S_{current best}
- F_{current best} = f(S_{current best})
- For temperature ...
  - generate S_{next}
  - F_{next} = f(S_{next})
  - if (F_{next} > F_{current best} || P(F_{next}, F_{current best}, T) < T)
    - S_{current best} \gets S_{next}
    - F_{current best} \gets F_{next}
